{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# cai_normalisation_entropy.py\n",
    "# Normalisation + reverse normalisation + ENTROPY weighting (Step-1 UNWEIGHTED) + CAI aggregation\n",
    "# + optional RICA-weighted population summaries (for reporting only)\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd, numpy as np\n",
    "from typing import List, Dict, Optional, Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ------------------- 3.7.1.1 Normalisation -------------------\n",
    "class MinMaxCAINormalizer:\n",
    "    def __init__(self, features: List[str], reverse_features: Optional[List[str]] = None,\n",
    "                 groupby: Optional[List[str]] = None, suffix: str = \"_n\"):\n",
    "        self.features = list(dict.fromkeys(features))\n",
    "        self.reverse_features = set(reverse_features or [])\n",
    "        self.groupby = groupby or []\n",
    "        self.suffix = suffix\n",
    "        self.params_: Optional[pd.DataFrame] = None\n",
    "\n",
    "    def _compute_params(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        miss = [c for c in self.features if c not in df.columns]\n",
    "        if miss:\n",
    "            raise KeyError(f\"Missing features: {miss}\")\n",
    "        if self.groupby:\n",
    "            g = df[self.groupby + self.features].groupby(self.groupby, dropna=False)\n",
    "            mins = g.min(numeric_only=True).rename(columns={c: f\"{c}__min\" for c in self.features})\n",
    "            maxs = g.max(numeric_only=True).rename(columns={c: f\"{c}__max\" for c in self.features})\n",
    "            params = mins.join(maxs, how=\"outer\").reset_index()\n",
    "        else:\n",
    "            mins = df[self.features].min(numeric_only=True).rename(lambda c: f\"{c}__min\")\n",
    "            maxs = df[self.features].max(numeric_only=True).rename(lambda c: f\"{c}__max\")\n",
    "            params = pd.concat([mins, maxs], axis=0).to_frame(\"value\").T\n",
    "        for f in self.features:\n",
    "            params[f\"{f}__range\"] = params[f\"{f}__max\"] - params[f\"{f}__min\"]\n",
    "            params[f\"{f}__is_constant\"] = params[f\"{f}__range\"].abs() < 1e-15\n",
    "        return params\n",
    "\n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        self.params_ = self._compute_params(df)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame, inplace: bool = False) -> pd.DataFrame:\n",
    "        if self.params_ is None:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "        out = df if inplace else df.copy()\n",
    "        if self.groupby:\n",
    "            merged = out.merge(self.params_, on=self.groupby, how=\"left\", validate=\"m:1\")\n",
    "        else:\n",
    "            tmp = self.params_.copy(); tmp[\"_cj\"] = 1; out[\"_cj\"] = 1\n",
    "            merged = out.merge(tmp, on=\"_cj\", how=\"left\").drop(columns=\"_cj\")\n",
    "            out.drop(columns=\"_cj\", inplace=True)\n",
    "        for f in self.features:\n",
    "            zcol = f\"{f}{self.suffix}\"\n",
    "            x = merged[f].astype(float)\n",
    "            xmin, rng, is_const = merged[f\"{f}__min\"], merged[f\"{f}__range\"], merged[f\"{f}__is_constant\"]\n",
    "            z = (x - xmin) / rng\n",
    "            z = np.where(is_const, 0.0, z)\n",
    "            z = np.clip(z, 0.0, 1.0)\n",
    "            if f in self.reverse_features:\n",
    "                z = 1.0 - z\n",
    "            out[zcol] = z\n",
    "        return out\n",
    "\n",
    "    def inverse_transform(self, df_norm: pd.DataFrame, inplace: bool = False) -> pd.DataFrame:\n",
    "        if self.params_ is None:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "        out = df_norm if inplace else df_norm.copy()\n",
    "        if self.groupby:\n",
    "            merged = out.merge(self.params_, on=self.groupby, how=\"left\", validate=\"m:1\")\n",
    "        else:\n",
    "            tmp = self.params_.copy(); tmp[\"_cj\"] = 1; out[\"_cj\"] = 1\n",
    "            merged = out.merge(tmp, on=\"_cj\", how=\"left\").drop(columns=\"_cj\")\n",
    "            out.drop(columns=\"_cj\", inplace=True)\n",
    "        for f in self.features:\n",
    "            zcol = f\"{f}{self.suffix}\"\n",
    "            if zcol not in merged.columns:\n",
    "                raise KeyError(f\"Missing {zcol}\")\n",
    "            z = merged[zcol].astype(float)\n",
    "            if f in self.reverse_features:\n",
    "                z = 1.0 - z\n",
    "            xmin, rng, is_const = merged[f\"{f}__min\"], merged[f\"{f}__range\"], merged[f\"{f}__is_constant\"]\n",
    "            x_rec = z * rng + xmin\n",
    "            x_rec = np.where(is_const, xmin, x_rec)\n",
    "            out[f\"{f}__recovered\"] = x_rec\n",
    "        return out\n",
    "\n",
    "    def report(self) -> pd.DataFrame:\n",
    "        if self.params_ is None:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "        rows = []\n",
    "        for _, r in self.params_.iterrows():\n",
    "            gvals = {g: r[g] for g in self.groupby} if self.groupby else {}\n",
    "            for f in self.features:\n",
    "                rows.append({\n",
    "                    **gvals, \"variable\": f,\n",
    "                    \"min\": r[f\"{f}__min\"], \"max\": r[f\"{f}__max\"], \"range\": r[f\"{f}__range\"],\n",
    "                    \"is_constant\": bool(r[f\"{f}__is_constant\"]), \"reversed\": (f in self.reverse_features)\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# --------------- 3.7.1.3 Entropy (Step-1) + 3.7.1.4 Aggregation ----------------\n",
    "class EntropyAggregator:\n",
    "    \"\"\"\n",
    "    Entropy weighting per eco-scheme (block) + aggregation.\n",
    "    If weight_col is provided, Step-1 proportions are WEIGHTED:\n",
    "        p_ij = w_i * z_ij / sum_i w_i * z_ij\n",
    "    Otherwise defaults to unweighted.\n",
    "    \"\"\"\n",
    "    def __init__(self, blocks_raw: Dict[str, List[str]], norm_suffix: str = \"_n\",\n",
    "                 groupby: Optional[List[str]] = None, eps: float = 1e-12, weight_col: Optional[str] = None):\n",
    "        self.blocks_raw = {k: list(dict.fromkeys(v)) for k, v in blocks_raw.items()}\n",
    "        self.norm_suffix = norm_suffix\n",
    "        self.groupby = groupby or []\n",
    "        self.eps = eps\n",
    "        self.weights_: Optional[pd.DataFrame] = None\n",
    "        self.weight_col = weight_col\n",
    "\n",
    "    def _norm_cols(self, raw_cols: Iterable[str]) -> List[str]:\n",
    "        return [f\"{c}{self.norm_suffix}\" for c in raw_cols]\n",
    "\n",
    "    def _entropy_weights_block(self, dfg: pd.DataFrame, block: str, cols_norm: List[str]) -> pd.DataFrame:\n",
    "        Z = dfg[cols_norm].copy()\n",
    "        mask_any = Z.notna().any(axis=1)\n",
    "        Z = Z.loc[mask_any].fillna(0.0)\n",
    "\n",
    "        # If all rows dropped â†’ return uniform\n",
    "        n_eff = len(Z)\n",
    "        if n_eff == 0:\n",
    "            return pd.DataFrame({\n",
    "                \"construct\": block, \"indicator\": cols_norm, \"H\": np.nan, \"d\": 0.0,\n",
    "                \"w\": 1.0 / max(len(cols_norm), 1)\n",
    "            })\n",
    "\n",
    "        # ----- Step 1: proportions (weighted if weight_col is provided) -----\n",
    "        if self.weight_col and self.weight_col in dfg.columns:\n",
    "            w = dfg.loc[Z.index, self.weight_col].fillna(0.0).clip(lower=0.0)\n",
    "            col_sums = (Z.mul(w, axis=0)).sum(axis=0).replace(0.0, np.nan)\n",
    "            P = Z.mul(w, axis=0).divide(col_sums, axis=1)\n",
    "        else:\n",
    "            col_sums = Z.sum(axis=0).replace(0.0, np.nan)\n",
    "            P = Z.divide(col_sums, axis=1)\n",
    "\n",
    "        for c in P.columns:\n",
    "            if pd.isna(col_sums[c]):\n",
    "                P[c] = 1.0 / n_eff  # uniform if column sum == 0\n",
    "\n",
    "        # ----- Step 2: entropy -----\n",
    "        k = 1.0 / np.log(max(n_eff, 2))\n",
    "        P_safe = P.clip(self.eps, 1.0)\n",
    "        H = (-k * (P_safe * np.log(P_safe)).sum(axis=0)).to_frame(name=\"H\")\n",
    "\n",
    "        # ----- Step 3: normalized weights -----\n",
    "        d = (1.0 - H[\"H\"]).clip(lower=0.0)\n",
    "        denom = d.sum()\n",
    "        wj = (d / denom) if denom > self.eps else pd.Series(np.full(len(d), 1.0 / len(d)), index=d.index)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"construct\": block,\n",
    "            \"indicator\": wj.index,\n",
    "            \"H\": H[\"H\"].values,\n",
    "            \"d\": d.values,\n",
    "            \"w\": wj.values\n",
    "        })\n",
    "\n",
    "    def fit(self, df_norm: pd.DataFrame):\n",
    "        needed = []\n",
    "        for _, raws in self.blocks_raw.items():\n",
    "            needed += self._norm_cols(raws)\n",
    "        miss = [c for c in set(needed) if c not in df_norm.columns]\n",
    "        if miss:\n",
    "            raise KeyError(f\"Missing normalized columns: {miss}\")\n",
    "\n",
    "        rows = []\n",
    "        if self.groupby:\n",
    "            for gkey, dfg in df_norm.groupby(self.groupby, dropna=False):\n",
    "                gdict = dict(zip(self.groupby, gkey if isinstance(gkey, tuple) else (gkey,)))\n",
    "                for block, raws in self.blocks_raw.items():\n",
    "                    cols = self._norm_cols(raws)\n",
    "                    wtbl = self._entropy_weights_block(dfg, block, cols)\n",
    "                    for k, v in gdict.items():\n",
    "                        wtbl[k] = v\n",
    "                    rows.append(wtbl)\n",
    "        else:\n",
    "            for block, raws in self.blocks_raw.items():\n",
    "                cols = self._norm_cols(raws)\n",
    "                rows.append(self._entropy_weights_block(df_norm, block, cols))\n",
    "        self.weights_ = pd.concat(rows, ignore_index=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_norm: pd.DataFrame, out_prefix: str = \"CAI_\") -> pd.DataFrame:\n",
    "        if self.weights_ is None:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "        out = df_norm.copy()\n",
    "        gcols = self.groupby\n",
    "\n",
    "        if gcols:\n",
    "            for block, raws in self.blocks_raw.items():\n",
    "                cols = self._norm_cols(raws)\n",
    "                wsub = self.weights_[self.weights_[\"construct\"] == block]\n",
    "                wwide = wsub.pivot_table(index=gcols, columns=\"indicator\", values=\"w\")\n",
    "                wwide = wwide.add_prefix(\"W::\").reset_index()\n",
    "                merged = out.merge(wwide, on=gcols, how=\"left\")\n",
    "                total = 0.0\n",
    "                for col in cols:\n",
    "                    wcol = \"W::\" + col\n",
    "                    if wcol not in merged.columns:\n",
    "                        merged[wcol] = 0.0\n",
    "                    total = total + merged[col].fillna(0.0) * merged[wcol].fillna(0.0)\n",
    "                out[f\"{out_prefix}{block}\"] = total\n",
    "        else:\n",
    "            for block, raws in self.blocks_raw.items():\n",
    "                cols = self._norm_cols(raws)\n",
    "                z = out[cols].copy().fillna(0.0)\n",
    "                w = (self.weights_[self.weights_[\"construct\"] == block]\n",
    "                     .set_index(\"indicator\")[\"w\"]).reindex(cols).fillna(0.0).values\n",
    "                out[f\"{out_prefix}{block}\"] = (z * w).sum(axis=1)\n",
    "        return out\n",
    "\n",
    "    def report_weights(self) -> pd.DataFrame:\n",
    "        if self.weights_ is None:\n",
    "            raise RuntimeError(\"fit() first\")\n",
    "        rep = self.weights_.copy()\n",
    "        rep[\"raw_variable\"] = rep[\"indicator\"].str.replace(self.norm_suffix + r\"$\", \"\", regex=True)\n",
    "        rep[\"step1_unweighted\"] = self.weight_col is None\n",
    "        return rep\n",
    "\n",
    "\n",
    "# ----------------------- Weighted summaries (RICA) -----------------------\n",
    "def weighted_mean(x: pd.Series, w: pd.Series) -> float:\n",
    "    x = x.astype(float); w = w.fillna(0.0).clip(lower=0.0).astype(float)\n",
    "    if w.sum() <= 0:\n",
    "        return float(x.mean())\n",
    "    return float(np.average(x, weights=w))\n",
    "\n",
    "\n",
    "def weighted_group_mean(df: pd.DataFrame, value_col: str, weight_col: str, groupby: List[str]) -> pd.DataFrame:\n",
    "    def agg(g):\n",
    "        return pd.Series({\n",
    "            \"weighted_mean\": weighted_mean(g[value_col], g[weight_col]),\n",
    "            \"n\": len(g),\n",
    "            \"sum_weights\": g[weight_col].fillna(0).sum()\n",
    "        })\n",
    "    return df.groupby(groupby, dropna=False).apply(agg).reset_index()\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Inputs\n",
    "# -------------------\n",
    "INFILE = \"data/processed/01_preprocessed_knn_pmm_ohe_te.csv\"\n",
    "REV_FEATURES = \"data/processed/reverse_features.csv\"\n",
    "OUT_NORM = \"data/processed/02_normalized.csv\"\n",
    "OUT_WEIGHTS = \"data/processed/entropy_weights.csv\"\n",
    "OUT_CAI = \"data/processed/final_cai_scores.csv\"\n",
    "\n",
    "PESO_COL = \"PESO\"\n",
    "ID_COLS = [\"Cod_Azienda\", \"Anno\"]\n",
    "\n",
    "# -------------------\n",
    "# Load\n",
    "# -------------------\n",
    "df = pd.read_csv(INFILE)\n",
    "df_rev_feat = pd.read_csv(REV_FEATURES)\n",
    "\n",
    "# Indicators = everything except IDs and PESO\n",
    "INDICATORS = [c for c in df.columns if c not in ID_COLS + [PESO_COL]]\n",
    "\n",
    "# -------------------\n",
    "# 1. Normalisation\n",
    "# -------------------\n",
    "normalizer = MinMaxCAINormalizer(features=INDICATORS, reverse_features=df_rev_feat.columns)\n",
    "df_norm = normalizer.fit(df).transform(df)\n",
    "\n",
    "# -------------------\n",
    "# 2. Entropy weights + CAI\n",
    "# -------------------\n",
    "# For now, treat all indicators as one block (\"ALL\")\n",
    "blocks = {\"ALL\": INDICATORS}\n",
    "\n",
    "aggregator = EntropyAggregator(blocks_raw=blocks, weight_col=\"PESO\")\n",
    "# Apply entropy calculations\n",
    "aggregator.fit(df_norm)\n",
    "# Apply weighting adjustments\n",
    "df_cai = aggregator.transform(df_norm)\n",
    "\n",
    "# -------------------\n",
    "# Save results\n",
    "# -------------------\n",
    "# Normalized indicators\n",
    "df_norm_out = pd.concat(\n",
    "    [df[ID_COLS + ([PESO_COL] if PESO_COL in df.columns else [])],\n",
    "     df_norm[[f\"{c}_n\" for c in INDICATORS]]],\n",
    "    axis=1\n",
    ")\n",
    "df_norm_out.to_csv(OUT_NORM, index=False)\n",
    "\n",
    "# Weights\n",
    "weights_df = aggregator.report_weights()\n",
    "weights_df.to_csv(OUT_WEIGHTS, index=False)\n",
    "\n",
    "# Final CAI scores (composite index)\n",
    "df_cai_out = pd.concat(\n",
    "    [df[ID_COLS + ([PESO_COL] if PESO_COL in df.columns else [])],\n",
    "     df_cai[[\"CAI_farm\"]]],\n",
    "    axis=1\n",
    ")\n",
    "df_cai_out.to_csv(OUT_CAI, index=False)\n",
    "\n",
    "# Also save CAI scores merged with original\n",
    "df_final = df_norm.copy()  # Start with normalised data\n",
    "df_final[\"CAI_farm\"] = df_cai[\"CAI_farm\"]  # Add CAI scores\n",
    "\n",
    "# Save complete dataset with CAI scores\n",
    "df_final.to_csv(\"data/processed/normalised_with_cai.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved normalized data, entropy weights, and CAI scores\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
