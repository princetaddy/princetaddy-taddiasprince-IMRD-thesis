{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "PLS-SEM variable screening for CAI (Type IV HOC, formativeâ€“formative, two-stage)\n",
    "\n",
    "Stage 1:\n",
    "  - Per-eco-scheme LOC (ES1..ES5) as formative constructs from proxy indicators\n",
    "  - Compute outer weights via PLS-PM (pyplspm)\n",
    "  - Bootstrap outer weights for significance (p-values)\n",
    "  - Multicollinearity screening via VIF per block (VIF > 5 -> flag)\n",
    "  - Drop indicators that fail significance and/or VIF threshold\n",
    "  - Refit Stage 1 and export final LOC scores\n",
    "\n",
    "Stage 2:\n",
    "  - Build HOC from LOC scores (single-indicator constructs for LOCs)\n",
    "  - Structural model: LOCs -> HOC\n",
    "  - Report RÂ²(HOC), fÂ² effects, and QÂ² via K-fold CV (Stoneâ€“Geisser)\n",
    "\n",
    "Outputs:\n",
    "  - reports/loc_vif_report.csv\n",
    "  - reports/outer_weights_bootstrap.csv\n",
    "  - reports/outer_weights_final.csv\n",
    "  - reports/stage2_paths_r2_f2.csv\n",
    "  - reports/stage2_q2_summary.csv\n",
    "  - data/loc_scores_stage1.csv\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# deps: pip install plspm statsmodels scikit-learn\n",
    "from plspm import Plspm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "DATA_CSV = \"data/processed/normalised_with_cai.csv\"  \n",
    "\n",
    "# Either (A) let the script auto-discover by suffix; or (B) specify explicit lists:\n",
    "AUTO_DISCOVER_BY_SUFFIX = True\n",
    "SUFFIXES = {\n",
    "    \"ES1\": \"_es1\",  # Eco-scheme 1\n",
    "    \"ES2\": \"_es2\",\n",
    "    \"ES3\": \"_es3\",\n",
    "    \"ES4\": \"_es4\",\n",
    "    \"ES5\": \"_es5\",\n",
    "}\n",
    "# If AUTO_DISCOVER_BY_SUFFIX is False, set blocks explicitly:\n",
    "EXPLICIT_BLOCKS = {\n",
    "    # \"ES1\": [\"varA_es1\", \"varB_es1\", ...],\n",
    "    # ...\n",
    "}\n",
    "\n",
    "GROUP_COLS = []    \n",
    "ID_COLS    = []    \n",
    "\n",
    "# Screening thresholds\n",
    "VIF_CUTOFF = 5.0\n",
    "ALPHA      = 0.05\n",
    "N_BOOT     = 1000      # bootstrap resamples for outer weight p-values\n",
    "RANDOM_SEED = 12345\n",
    "N_SPLITS_Q2 = 10       # K-fold for QÂ²\n",
    "\n",
    "# Output dirs\n",
    "Path(\"reports\").mkdir(exist_ok=True)\n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "def discover_blocks_by_suffix(df: pd.DataFrame, suffix_map: Dict[str, str]) -> Dict[str, List[str]]:\n",
    "    blocks = {}\n",
    "    for loc, suf in suffix_map.items():\n",
    "        cols = [c for c in df.columns if c.endswith(suf)]\n",
    "        blocks[loc] = cols\n",
    "    return blocks\n",
    "\n",
    "def make_path_matrix_stage1(locs: List[str]) -> pd.DataFrame:\n",
    "    # No structural paths in stage-1 (just composites), pyplspm needs a square matrix of zeros\n",
    "    P = pd.DataFrame(0, index=locs, columns=locs, dtype=int)\n",
    "    return P\n",
    "\n",
    "def make_path_matrix_stage2(locs: List[str], hoc: str = \"HOC\") -> pd.DataFrame:\n",
    "    # LOCs -> HOC\n",
    "    nodes = locs + [hoc]\n",
    "    P = pd.DataFrame(0, index=nodes, columns=nodes, dtype=int)\n",
    "    for l in locs:\n",
    "        P.loc[hoc, l] = 1\n",
    "    return P\n",
    "\n",
    "def block_modes(n_blocks: int, mode: str = \"B\") -> List[str]:\n",
    "    # \"B\" for formative, \"A\" for reflective\n",
    "    return [mode] * n_blocks\n",
    "\n",
    "def compute_vif_table(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_ = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if X_.shape[1] < 2 or X_.shape[0] < 5:\n",
    "        return pd.DataFrame({\"variable\": X.columns, \"VIF\": np.nan})\n",
    "    X_mat = X_.values\n",
    "    vif_vals = []\n",
    "    for j in range(X_mat.shape[1]):\n",
    "        vif_vals.append(variance_inflation_factor(X_mat, j))\n",
    "    return pd.DataFrame({\"variable\": X_.columns, \"VIF\": vif_vals})\n",
    "\n",
    "def bootstrap_outer_weights(\n",
    "    df_block: pd.DataFrame,\n",
    "    construct_name: str,\n",
    "    n_boot: int = 1000,\n",
    "    random_state: int = 12345,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bootstraps outer weights for a single formative block using pyplspm by\n",
    "    running a tiny PLS model with only that construct (no inner paths).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    items = list(df_block.columns)\n",
    "    if len(items) == 0:\n",
    "        return pd.DataFrame(columns=[\"construct\", \"indicator\", \"weight_mean\", \"weight_se\", \"t\", \"p\"])\n",
    "\n",
    "    # Prepare a trivial PLS model: one construct measured by formative indicators\n",
    "    P = pd.DataFrame(0, index=[construct_name], columns=[construct_name], dtype=int)\n",
    "    blocks = {construct_name: items}\n",
    "    modes  = [\"B\"]\n",
    "\n",
    "    # First run (original sample) for reference weights\n",
    "    model_ref = Plspm(df_block, P, blocks, modes, scheme=\"path\", bootval=False)\n",
    "    w_ref = model_ref.outer_model_[[\"name\", \"weight\"]].copy().rename(columns={\"name\": \"indicator\"})\n",
    "    w_ref[\"construct\"] = construct_name\n",
    "\n",
    "    # Bootstrap\n",
    "    W = []\n",
    "    n = len(df_block)\n",
    "    for _ in range(n_boot):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        sample = df_block.iloc[idx].reset_index(drop=True)\n",
    "        try:\n",
    "            m = Plspm(sample, P, blocks, modes, scheme=\"path\", bootval=False)\n",
    "            ow = m.outer_model_[[\"name\", \"weight\"]].copy()\n",
    "            ow[\"construct\"] = construct_name\n",
    "            W.append(ow.rename(columns={\"name\": \"indicator\"}))\n",
    "        except Exception:\n",
    "            # If a bootstrap sample fails (e.g., singular), skip\n",
    "            continue\n",
    "    if not W:\n",
    "        # Fallback: return reference weights only\n",
    "        w_ref[\"weight_mean\"] = w_ref[\"weight\"]\n",
    "        w_ref[\"weight_se\"] = np.nan\n",
    "        w_ref[\"t\"] = np.nan\n",
    "        w_ref[\"p\"] = np.nan\n",
    "        return w_ref[[\"construct\",\"indicator\",\"weight_mean\",\"weight_se\",\"t\",\"p\"]]\n",
    "\n",
    "    Wdf = pd.concat(W, ignore_index=True)\n",
    "    stats_tbl = (\n",
    "        Wdf.groupby([\"construct\", \"indicator\"])[\"weight\"]\n",
    "           .agg([\"mean\", \"std\", \"count\"])\n",
    "           .reset_index()\n",
    "           .rename(columns={\"mean\":\"weight_mean\",\"std\":\"weight_se\"})\n",
    "    )\n",
    "\n",
    "    # t-stats vs 0, p-values (two-sided)\n",
    "    stats_tbl[\"t\"] = stats_tbl[\"weight_mean\"] / stats_tbl[\"weight_se\"]\n",
    "    stats_tbl[\"p\"] = 2 * stats.t.sf(np.abs(stats_tbl[\"t\"]), df=stats_tbl[\"count\"] - 1)\n",
    "\n",
    "    return stats_tbl[[\"construct\", \"indicator\", \"weight_mean\", \"weight_se\", \"t\", \"p\"]]\n",
    "\n",
    "def compute_f2_effects(df: pd.DataFrame, y: str, X_full: List[str]) -> pd.DataFrame:\n",
    "    \"\"\" Cohen's fÂ² = (RÂ²_included - RÂ²_excluded) / (1 - RÂ²_included) for each predictor. \"\"\"\n",
    "    lr = LinearRegression()\n",
    "    # Full model R2\n",
    "    lr.fit(df[X_full], df[y])\n",
    "    r2_full = lr.score(df[X_full], df[y])\n",
    "\n",
    "    rows = []\n",
    "    for x in X_full:\n",
    "        X_red = [v for v in X_full if v != x]\n",
    "        lr.fit(df[X_red], df[y])\n",
    "        r2_red = lr.score(df[X_red], df[y])\n",
    "        f2 = (r2_full - r2_red) / (1 - r2_full + 1e-12)\n",
    "        rows.append({\"predictor\": x, \"R2_full\": r2_full, \"R2_reduced\": r2_red, \"f2\": f2})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def compute_q2(df: pd.DataFrame, y: str, X: List[str], n_splits: int = 10, seed: int = 12345) -> Tuple[float, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Stoneâ€“Geisser QÂ² via K-fold CV on the endogenous HOC indicator.\n",
    "    QÂ² = 1 - PRESS/SST; also return fold-by-fold details.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    y_true_all, y_pred_all, fold_rows = [], [], []\n",
    "    lr = LinearRegression()\n",
    "    for k, (tr, te) in enumerate(kf.split(df), 1):\n",
    "        Xtr, Xte = df.iloc[tr][X], df.iloc[te][X]\n",
    "        ytr, yte = df.iloc[tr][y], df.iloc[te][y]\n",
    "        lr.fit(Xtr, ytr)\n",
    "        yhat = lr.predict(Xte)\n",
    "        y_true_all.append(yte)\n",
    "        y_pred_all.append(yhat)\n",
    "        fold_rows.append(pd.DataFrame({\"fold\": k, \"y_true\": yte.values, \"y_pred\": yhat}))\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "    press = np.sum((y_true - y_pred)**2)\n",
    "    sst   = np.sum((y_true - np.mean(y_true))**2)\n",
    "    q2 = 1 - press / (sst + 1e-12)\n",
    "    q2_df = pd.concat(fold_rows, ignore_index=True)\n",
    "    return q2, q2_df\n",
    "\n",
    "\n",
    "# -------------------- MAIN PIPELINE --------------------\n",
    "def main():\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "    # Build blocks (LOC -> indicators)\n",
    "    if AUTO_DISCOVER_BY_SUFFIX:\n",
    "        blocks = discover_blocks_by_suffix(df, SUFFIXES)\n",
    "    else:\n",
    "        blocks = EXPLICIT_BLOCKS\n",
    "\n",
    "    # Keep only rows with all needed indicators present (drop rows with all-NA per LOC)\n",
    "    for loc, cols in blocks.items():\n",
    "        if not cols:\n",
    "            raise ValueError(f\"No indicators found for {loc}. Provide explicit lists or adjust suffixes.\")\n",
    "        # allow some missing but drop rows where *all* LOC indicators are NA\n",
    "        df = df[df[cols].notna().any(axis=1)]\n",
    "\n",
    "    # Optional: run per-group\n",
    "    if GROUP_COLS:\n",
    "        groups = df.groupby(GROUP_COLS, dropna=False)\n",
    "    else:\n",
    "        groups = [((), df)]  # single pooled group\n",
    "\n",
    "    loc_scores_all = []\n",
    "    outer_boot_all = []\n",
    "    vif_all = []\n",
    "    outer_final_all = []\n",
    "\n",
    "    for gkey, dfg in groups:\n",
    "        # --- Stage 1: per LOC ---\n",
    "        locs = list(blocks.keys())\n",
    "        P1 = make_path_matrix_stage1(locs)\n",
    "        modes1 = block_modes(len(locs), \"B\")\n",
    "\n",
    "        # VIF screening & bootstrap per LOC\n",
    "        kept_blocks = {}\n",
    "        for loc, cols in blocks.items():\n",
    "            X = dfg[cols].copy()\n",
    "            vif_tbl = compute_vif_table(X)\n",
    "            vif_tbl.insert(0, \"group\", str(gkey))\n",
    "            vif_tbl.insert(1, \"construct\", loc)\n",
    "            vif_all.append(vif_tbl)\n",
    "\n",
    "            # Bootstrap outer weights\n",
    "            boot_tbl = bootstrap_outer_weights(X, loc, n_boot=N_BOOT, random_state=RANDOM_SEED)\n",
    "            boot_tbl.insert(0, \"group\", str(gkey))\n",
    "            outer_boot_all.append(boot_tbl)\n",
    "\n",
    "            # Apply screening: keep indicators with p<=alpha and VIF<=cutoff (if VIF available)\n",
    "            merge_scr = boot_tbl.merge(vif_tbl.rename(columns={\"variable\":\"indicator\"}), on=[\"construct\",\"indicator\"], how=\"left\")\n",
    "            keep_mask = (merge_scr[\"p\"] <= ALPHA) & ((merge_scr[\"VIF\"].isna()) | (merge_scr[\"VIF\"] <= VIF_CUTOFF))\n",
    "            keep_inds = merge_scr.loc[keep_mask, \"indicator\"].tolist()\n",
    "\n",
    "            # If all dropped, fallback: keep the strongest (lowest p) to avoid empty block\n",
    "            if len(keep_inds) == 0 and len(cols) > 0:\n",
    "                best = merge_scr.sort_values(\"p\", ascending=True).iloc[0][\"indicator\"]\n",
    "                keep_inds = [best]\n",
    "\n",
    "            kept_blocks[loc] = keep_inds\n",
    "\n",
    "        # Refit Stage 1 with kept indicators\n",
    "        P1_refit = make_path_matrix_stage1(locs)\n",
    "        modes1_refit = block_modes(len(locs), \"B\")\n",
    "\n",
    "        model1 = Plspm(\n",
    "            dfg,\n",
    "            P1_refit,\n",
    "            kept_blocks,\n",
    "            modes1_refit,\n",
    "            scheme=\"path\",\n",
    "            bootval=False\n",
    "        )\n",
    "\n",
    "        # Save final outer weights (after screening)\n",
    "        om = model1.outer_model_[[\"name\",\"block\",\"weight\"]].rename(columns={\"name\":\"indicator\",\"block\":\"construct\"})\n",
    "        om.insert(0, \"group\", str(gkey))\n",
    "        outer_final_all.append(om)\n",
    "\n",
    "        # Extract LOC scores\n",
    "        scores = model1.scores_.copy()\n",
    "        if ID_COLS:\n",
    "            scores = pd.concat([dfg[ID_COLS].reset_index(drop=True), scores.reset_index(drop=True)], axis=1)\n",
    "        scores.insert(0, \"group\", str(gkey))\n",
    "        loc_scores_all.append(scores)\n",
    "\n",
    "    # Concatenate and save Stage 1 reports\n",
    "    pd.concat(vif_all, ignore_index=True).to_csv(\"reports/loc_vif_report.csv\", index=False)\n",
    "    pd.concat(outer_boot_all, ignore_index=True).to_csv(\"reports/outer_weights_bootstrap.csv\", index=False)\n",
    "    pd.concat(outer_final_all, ignore_index=True).to_csv(\"reports/outer_weights_final.csv\", index=False)\n",
    "    loc_scores = pd.concat(loc_scores_all, ignore_index=True)\n",
    "    loc_scores.to_csv(\"data/loc_scores_stage1.csv\", index=False)\n",
    "\n",
    "    # --- Stage 2: LOC scores -> HOC ---\n",
    "    # Build a synthetic single-indicator for HOC as the mean of LOC scores (for measurement anchor)\n",
    "    loc_cols = list(blocks.keys())\n",
    "    df2 = loc_scores.copy()\n",
    "    df2[\"HOC_indicator\"] = df2[loc_cols].mean(axis=1)\n",
    "\n",
    "    # Build stage-2 path matrix: LOCs -> HOC\n",
    "    P2 = make_path_matrix_stage2(loc_cols, hoc=\"HOC\")\n",
    "\n",
    "    # Measurement model:\n",
    "    #  - Each LOC is a single-indicator construct (reflective \"A\") using its own score column.\n",
    "    #  - HOC is a single-indicator construct measured by HOC_indicator (mode can be \"B\" or \"A\"â€”single item).\n",
    "    blocks2 = {loc: [loc] for loc in loc_cols}\n",
    "    blocks2[\"HOC\"] = [\"HOC_indicator\"]\n",
    "    modes2 = [\"A\"] * len(loc_cols) + [\"B\"]\n",
    "\n",
    "    model2 = Plspm(\n",
    "        df2,\n",
    "        P2,\n",
    "        blocks2,\n",
    "        modes2,\n",
    "        scheme=\"path\",\n",
    "        bootval=False\n",
    "    )\n",
    "\n",
    "    # Paths & R2\n",
    "    inner = model2.path_coefficients_.reset_index().rename(columns={\"index\":\"to\"})\n",
    "    r2 = model2.r2_.reset_index().rename(columns={\"index\":\"construct\", 0:\"R2\"})\n",
    "\n",
    "    # fÂ² effects (OLS on HOC_indicator ~ LOCs)\n",
    "    f2_tbl = compute_f2_effects(df2, y=\"HOC_indicator\", X_full=loc_cols)\n",
    "\n",
    "    # QÂ² via K-fold CV (predict HOC_indicator from LOCs)\n",
    "    q2, q2_folds = compute_q2(df2, y=\"HOC_indicator\", X=loc_cols, n_splits=N_SPLITS_Q2, seed=RANDOM_SEED)\n",
    "\n",
    "    # Save stage-2 reports\n",
    "    stage2_paths = inner.melt(id_vars=\"to\", var_name=\"from\", value_name=\"beta\")\n",
    "    stage2_paths = stage2_paths[stage2_paths[\"to\"] == \"HOC\"][[\"from\",\"to\",\"beta\"]]\n",
    "    stage2_paths = stage2_paths.merge(f2_tbl.rename(columns={\"predictor\":\"from\"}), on=\"from\", how=\"left\")\n",
    "\n",
    "    # Attach R2(HOC)\n",
    "    r2_hoc = float(r2.loc[r2[\"construct\"]==\"HOC\",\"R2\"])\n",
    "    stage2_paths[\"R2_HOC\"] = r2_hoc\n",
    "\n",
    "    stage2_paths.to_csv(\"reports/stage2_paths_r2_f2.csv\", index=False)\n",
    "\n",
    "    pd.DataFrame({\"metric\":[\"Q2_HOC\"], \"value\":[q2]}).to_csv(\"reports/stage2_q2_summary.csv\", index=False)\n",
    "    q2_folds.to_csv(\"reports/stage2_q2_folds.csv\", index=False)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print(f\"RÂ²(HOC) = {r2_hoc:.3f} | QÂ²(HOC) = {q2:.3f}\")\n",
    "    print(\"Key files written to ./reports and ./data\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "77a75a4ae1df9fe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Export SEM results to CSV tables for visualization\n",
    "This script reads the PLS-SEM results and formats them into the CSV tables\n",
    "expected by the visualization script (table1_formative.csv and table2_structural.csv)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def export_sem_results_to_csv():\n",
    "    \"\"\"\n",
    "    Reads PLS-SEM results and exports them as CSV tables for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the PLS-SEM results from your pipeline\n",
    "    outer_weights_final = pd.read_csv(\"reports/outer_weights_final.csv\")\n",
    "    stage2_paths = pd.read_csv(\"reports/stage2_paths_r2_f2.csv\")\n",
    "\n",
    "    # ==================== TABLE 1: Formative Measurement Model ====================\n",
    "    # Format: Construct | Indicator | Weight\n",
    "    table1 = outer_weights_final.copy()\n",
    "\n",
    "    # Clean up column names to match expected format\n",
    "    table1_formatted = pd.DataFrame({\n",
    "        'Construct': table1['construct'],\n",
    "        'Indicator': table1['indicator'],\n",
    "        'Weight': table1['weight'].round(3)\n",
    "    })\n",
    "\n",
    "    # Sort by construct then by weight (descending)\n",
    "    table1_formatted = table1_formatted.sort_values(['Construct', 'Weight'],\n",
    "                                                  ascending=[True, False])\n",
    "\n",
    "    # Save Table 1\n",
    "    table1_formatted.to_csv(\"table1_formative.csv\", index=False)\n",
    "    print(f\"âœ“ Saved table1_formative.csv with {len(table1_formatted)} indicators\")\n",
    "\n",
    "\n",
    "    # ==================== TABLE 2: Structural Model ====================\n",
    "    # Format: Path | Î²_std (or beta_std) | [other stats]\n",
    "    table2_formatted = pd.DataFrame({\n",
    "        'Path': stage2_paths['from'] + ' â†’ CAI',  # Assuming CAI is your outcome\n",
    "        'Î²_std': stage2_paths['beta'].round(3),\n",
    "        'R2_full': stage2_paths['R2_HOC'].round(3),\n",
    "        'f2': stage2_paths['f2'].round(3)\n",
    "    })\n",
    "\n",
    "    # Add significance indicators if you have p-values\n",
    "    # (You might need to add these from bootstrap results if available)\n",
    "\n",
    "    # Save Table 2\n",
    "    table2_formatted.to_csv(\"table2_structural.csv\", index=False)\n",
    "    print(f\"âœ“ Saved table2_structural.csv with {len(table2_formatted)} paths\")\n",
    "\n",
    "\n",
    "    # ==================== SUMMARY REPORT ====================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SEM RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\nFORMATIVE MEASUREMENT MODEL:\")\n",
    "    print(f\"Total constructs: {table1_formatted['Construct'].nunique()}\")\n",
    "    for construct in table1_formatted['Construct'].unique():\n",
    "        n_indicators = len(table1_formatted[table1_formatted['Construct'] == construct])\n",
    "        avg_weight = table1_formatted[table1_formatted['Construct'] == construct]['Weight'].mean()\n",
    "        print(f\"  {construct}: {n_indicators} indicators (avg weight: {avg_weight:.3f})\")\n",
    "\n",
    "    print(f\"\\nSTRUCTURAL MODEL:\")\n",
    "    hoc_r2 = stage2_paths['R2_HOC'].iloc[0] if len(stage2_paths) > 0 else 0\n",
    "    print(f\"  RÂ² (Higher-Order Construct): {hoc_r2:.3f}\")\n",
    "    print(f\"  Number of paths to CAI: {len(table2_formatted)}\")\n",
    "\n",
    "    # Show path coefficients\n",
    "    for _, row in table2_formatted.iterrows():\n",
    "        print(f\"    {row['Path']}: Î² = {row['Î²_std']:.3f} (fÂ² = {row['f2']:.3f})\")\n",
    "\n",
    "    print(f\"\\nâœ“ Files ready for visualization script\")\n",
    "\n",
    "\n",
    "def validate_csv_for_visualization():\n",
    "    \"\"\"\n",
    "    Validate that the CSV files are properly formatted for the visualization script\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check table1\n",
    "        t1 = pd.read_csv(\"table1_formative.csv\")\n",
    "        required_cols_t1 = ['Construct', 'Indicator']\n",
    "        weight_col = \"Weight\" if \"Weight\" in t1.columns else (\"Loading\" if \"Loading\" in t1.columns else None)\n",
    "\n",
    "        if weight_col is None:\n",
    "            print(\"âš ï¸  Warning: No Weight or Loading column found in table1\")\n",
    "        else:\n",
    "            print(f\"âœ“ Table1 validated: {len(t1)} rows, weight column: '{weight_col}'\")\n",
    "\n",
    "        # Check table2\n",
    "        t2 = pd.read_csv(\"table2_structural.csv\")\n",
    "        beta_candidates = [\"Î²_std\", \"beta_std\", \"Std. Beta\", \"Std. Î²\", \"Î²\", \"beta\", \"Path Coefficient\"]\n",
    "        beta_col = next((c for c in beta_candidates if c in t2.columns), None)\n",
    "\n",
    "        if beta_col is None:\n",
    "            print(\"âš ï¸  Warning: No beta coefficient column found in table2\")\n",
    "        else:\n",
    "            print(f\"âœ“ Table2 validated: {len(t2)} rows, beta column: '{beta_col}'\")\n",
    "\n",
    "        if \"Path\" in t2.columns:\n",
    "            print(f\"âœ“ Path column found in table2\")\n",
    "        elif \"Hypothesis\" in t2.columns:\n",
    "            print(f\"âœ“ Hypothesis column found in table2 (will be used as Path)\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Warning: No Path or Hypothesis column in table2\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ File not found: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Validation error: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if input files exist\n",
    "    required_files = [\n",
    "        \"reports/outer_weights_final.csv\",\n",
    "        \"reports/stage2_paths_r2_f2.csv\"\n",
    "    ]\n",
    "\n",
    "    missing_files = [f for f in required_files if not Path(f).exists()]\n",
    "\n",
    "    if missing_files:\n",
    "        print(\"âŒ Missing required input files:\")\n",
    "        for f in missing_files:\n",
    "            print(f\"   {f}\")\n",
    "        print(\"\\nðŸ’¡ Run PLS-SEM pipeline\")\n",
    "    else:\n",
    "        print(\"âœ“ Found required input files\")\n",
    "        export_sem_results_to_csv()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        validate_csv_for_visualization()"
   ],
   "id": "6e7ba8275217a3f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate diagram with indicator labels colored to match arrow/construct\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, FancyArrowPatch, Wedge\n",
    "\n",
    "# reload data\n",
    "t1 = pd.read_csv(\"table1_formative.csv\")\n",
    "t2 = pd.read_csv(\"table2_structural.csv\")\n",
    "\n",
    "weight_col = \"Weight\" if \"Weight\" in t1.columns else (\"Loading\" if \"Loading\" in t1.columns else None)\n",
    "blocks = {}\n",
    "for _, r in t1.iterrows():\n",
    "    c = str(r[\"Construct\"]).strip()\n",
    "    ind = str(r[\"Indicator\"]).strip()\n",
    "    w = float(r[weight_col])\n",
    "    blocks.setdefault(c, []).append({\"indicator\": ind, \"weight\": w})\n",
    "\n",
    "beta_candidates = [\"Î²_std\", \"beta_std\", \"Std. Beta\", \"Std. Î²\", \"Î²\", \"beta\", \"Path Coefficient\"]\n",
    "beta_col = next((c for c in beta_candidates if c in t2.columns), None)\n",
    "\n",
    "def to_cai(path_str: str) -> bool:\n",
    "    if not isinstance(path_str, str): return False\n",
    "    s = path_str.replace(\"->\", \"â†’\")\n",
    "    return (\"â†’\" in s) and (s.split(\"â†’\")[-1].strip().upper().startswith(\"CAI\"))\n",
    "\n",
    "beta_rows = t2[t2[\"Path\"].apply(to_cai)] if \"Path\" in t2.columns else t2.copy()\n",
    "betas = {}\n",
    "if not beta_rows.empty and \"Path\" in beta_rows.columns:\n",
    "    for _, r in beta_rows.iterrows():\n",
    "        p = str(r[\"Path\"]).replace(\"->\", \"â†’\")\n",
    "        src = p.split(\"â†’\")[0].strip()\n",
    "        betas[src] = float(r[beta_col])\n",
    "elif \"Hypothesis\" in t2.columns:\n",
    "    for _, r in t2.iterrows():\n",
    "        h = str(r[\"Hypothesis\"]).replace(\"->\", \"â†’\")\n",
    "        if to_cai(h):\n",
    "            src = h.split(\"â†’\")[0].strip()\n",
    "            betas[src] = float(r[beta_col])\n",
    "\n",
    "constructs = [k for k in [\"ES1\",\"ES2\",\"ES3\",\"ES4\",\"ES5\"] if k in blocks and k in betas]\n",
    "constructs += [k for k in blocks.keys() if k not in constructs and k in betas]\n",
    "\n",
    "# styling\n",
    "COL = {\n",
    "    \"ES1\": \"#2A6FDB\",\n",
    "    \"ES2\": \"#2B7A2A\",\n",
    "    \"ES3\": \"#BB6B00\",\n",
    "    \"ES4\": \"#6F3DB2\",\n",
    "    \"ES5\": \"#B53A3A\",\n",
    "}\n",
    "SECTOR_ALPHA = 0.06\n",
    "\n",
    "R_ES    = 2.2\n",
    "R_RING  = 3.5\n",
    "R_LABEL = 4.3\n",
    "ES_NODE_R = 0.32\n",
    "CAI_NODE_R = 0.45\n",
    "SECTOR_WIDTH_DEG = 65\n",
    "\n",
    "def pol2cart(r, theta):\n",
    "    return r*np.cos(theta), r*np.sin(theta)\n",
    "\n",
    "def nice_angle_text_rotation(theta):\n",
    "    ang = np.degrees(theta) % 360\n",
    "    if 90 < ang < 270:\n",
    "        return ang + 180, \"right\"\n",
    "    else:\n",
    "        return ang, \"left\"\n",
    "\n",
    "def arrow_with_text(ax, start, end, text, color=\"#444\", lw=2.0):\n",
    "    arrow = FancyArrowPatch(start, end, arrowstyle=\"-|>\", mutation_scale=13,\n",
    "                            lw=lw, color=color, connectionstyle=\"arc3,rad=0\")\n",
    "    ax.add_patch(arrow)\n",
    "    mx = (0.6*end[0] + 0.4*start[0])\n",
    "    my = (0.6*end[1] + 0.4*start[1])\n",
    "    ax.text(mx, my, text, fontsize=9, color=color, ha=\"center\", va=\"center\",\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"none\", pad=0.2, alpha=0.7))\n",
    "\n",
    "def arrow_touching_circle(ax, start_xy, circle_center, circle_radius, color=\"#444\", lw=1.6, rad=0.2):\n",
    "    sx, sy = start_xy\n",
    "    cx, cy = circle_center\n",
    "    vx, vy = cx - sx, cy - sy\n",
    "    L = (vx*vx + vy*vy) ** 0.5 + 1e-12\n",
    "    ux, uy = vx/L, vy/L\n",
    "    ex, ey = cx - ux*circle_radius, cy - uy*circle_radius\n",
    "    patch = FancyArrowPatch((sx,sy), (ex,ey),\n",
    "                            connectionstyle=f\"arc3,rad={rad}\",\n",
    "                            arrowstyle=\"-|>\", mutation_scale=11, lw=lw, color=color)\n",
    "    ax.add_patch(patch)\n",
    "\n",
    "def point_toward(p_from, p_to, delta):\n",
    "    vx, vy = p_to[0]-p_from[0], p_to[1]-p_from[1]\n",
    "    L = (vx*vx + vy*vy) ** 0.5 + 1e-12\n",
    "    ux, uy = vx/L, vy/L\n",
    "    return (p_from[0] + ux*delta, p_from[1] + uy*delta)\n",
    "\n",
    "# figure\n",
    "fig, ax = plt.subplots(figsize=(13, 13))\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "theta_grid = np.linspace(0, 2*np.pi, 720)\n",
    "x, y = pol2cart(R_RING, theta_grid)\n",
    "ax.plot(x, y, lw=1.0, color=\"#888\")\n",
    "\n",
    "cai = Circle((0,0), CAI_NODE_R, facecolor=\"#EEEEEE\", edgecolor=\"#333\", lw=2)\n",
    "ax.add_patch(cai)\n",
    "ax.text(0, 0, \"Overall Adoption\\n(CAI)\", ha=\"center\", va=\"center\", fontsize=12, color=\"#111\")\n",
    "\n",
    "N = len(constructs)\n",
    "base_angles = np.linspace(np.radians(90), np.radians(90) - 2*np.pi + 2*np.pi/N, N)\n",
    "pos_ES = {c: pol2cart(R_ES, base_angles[i]) for i, c in enumerate(constructs)}\n",
    "\n",
    "for i, c in enumerate(constructs):\n",
    "    theta = base_angles[i]\n",
    "    cx, cy = pos_ES[c]\n",
    "    color = COL.get(c, \"#444\")\n",
    "    \n",
    "    half = np.radians(SECTOR_WIDTH_DEG/2)\n",
    "    sector = Wedge(center=(0,0), r=R_RING+0.05,\n",
    "                   theta1=np.degrees(theta-half), theta2=np.degrees(theta+half),\n",
    "                   width=0.50, facecolor=color, alpha=SECTOR_ALPHA, edgecolor=None)\n",
    "    ax.add_patch(sector)\n",
    "    \n",
    "    node = Circle((cx, cy), ES_NODE_R, facecolor=\"#FFFFFF\", edgecolor=color, lw=2.2)\n",
    "    ax.add_patch(node)\n",
    "    ax.text(cx, cy, c, ha=\"center\", va=\"center\", fontsize=12, color=color)\n",
    "    \n",
    "    start_beta = point_toward((cx,cy), (0,0), ES_NODE_R)\n",
    "    end_beta   = point_toward((0,0), (cx,cy), CAI_NODE_R)\n",
    "    arrow_with_text(ax, start_beta, end_beta, f\"Î²={float(betas[c]):.2f}\", color=color, lw=2.3)\n",
    "    \n",
    "    inds = blocks.get(c, [])\n",
    "    if not inds: \n",
    "        continue\n",
    "    K = len(inds)\n",
    "    if K == 1:\n",
    "        thetas = np.array([theta])\n",
    "    else:\n",
    "        thetas = np.linspace(theta - half*1.0, theta + half*1.0, K)\n",
    "    \n",
    "    for j, info in enumerate(inds):\n",
    "        t = thetas[j]\n",
    "        ix, iy = pol2cart(R_RING, t)\n",
    "        ax.plot([ix], [iy], marker=\"o\", ms=3.0, color=color, zorder=3)\n",
    "        \n",
    "        rot_deg, align = nice_angle_text_rotation(t)\n",
    "        tx, ty = pol2cart(R_LABEL, t)\n",
    "        ax.text(tx, ty, f\"Ï‰={info['weight']:.2f} Â· {info['indicator']}\",\n",
    "                rotation=rot_deg, ha=align, va=\"center\", fontsize=8.8, color=color)\n",
    "        \n",
    "        ax.plot([ix, tx], [iy, ty], lw=0.8, color=color, alpha=0.6)\n",
    "        \n",
    "        rad = 0.25 if align == \"left\" else -0.25\n",
    "        arrow_touching_circle(ax, (ix,iy), (cx,cy), ES_NODE_R, color=color, lw=1.4, rad=rad)\n",
    "\n",
    "pad = R_LABEL + 1.6\n",
    "ax.set_xlim(-pad, pad); ax.set_ylim(-pad, pad)\n",
    "\n",
    "out_png = \"/mnt/data/pls_phylo_ring_v5.png\"\n",
    "out_pdf = \"/mnt/data/pls_phylo_ring_v5.pdf\"\n",
    "fig.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "fig.savefig(out_pdf, bbox_inches=\"tight\")\n",
    "out_png, out_pdf\n"
   ],
   "id": "deac578877fb9d07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
