{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature_engineering_knn_pmm.py\n",
    "# ---------------------------------------------------------------\n",
    "# 3.7.4 Feature Engineering\n",
    "# - Merge many raw CSVs on keys\n",
    "# - KNN imputation for CATEGORICALS (mode of neighbours by numeric distance)\n",
    "# - PMM imputation for CONTINUOUS (Ridge-based predictive mean matching)\n",
    "# - One-Hot + leakage-safe K-fold Target Encoding for categoricals\n",
    "# - Saves a single modelling table with ID_COLS preserved\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "import glob, math, warnings\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ---------------------- CONFIG ----------------------\n",
    "RAW_GLOB = \"data/raw/*.csv\"\n",
    "OUT_DIR  = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ID_COLS   = [\"Cod_Azienda\", \"Anno\"]   # <-- merge keys\n",
    "TARGET    = \"target_var\"              # <-- binary 0/1 target for target encoding\n",
    "\n",
    "# Variable types.\n",
    "CATEGORICALS: List[str] = []\n",
    "NUMERICALS:   List[str] = []\n",
    "\n",
    "#Missing Values\n",
    "# KNN for categorical imputation\n",
    "KNN_K = 5\n",
    "\n",
    "# PMM for continuous imputation\n",
    "PMM_K = 5\n",
    "PMM_ALPHA = 1.0     # Ridge regularization for the predictive model\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def merge_on_keys(dfs: List[pd.DataFrame], on: List[str]) -> pd.DataFrame:\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=on)\n",
    "    return reduce(lambda L, R: pd.merge(L, R, on=on, how=\"outer\"), dfs)\n",
    "\n",
    "def infer_types(df: pd.DataFrame, id_cols: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    cats = [c for c in df.columns if df[c].dtype == \"object\" and c not in id_cols]\n",
    "    nums = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c not in id_cols]\n",
    "    return cats, nums\n",
    "\n",
    "# ---------- KNN imputation for CATEGORICALS (mode of NN) ----------\n",
    "def knn_impute_categoricals(df: pd.DataFrame, categorical_cols: List[str], numeric_cols: List[str], k: int=5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputing missing categories using the mode of K nearest neighbours in numeric space.\n",
    "    - Distances computed on numeric predictors (median-filled).\n",
    "    - For each categorical column independently, donors are rows where that column is observed.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # numeric matrix for distances (median imputed)\n",
    "    if numeric_cols:\n",
    "        num_imp = SimpleImputer(strategy=\"median\")\n",
    "        X_num = num_imp.fit_transform(out[numeric_cols])\n",
    "    else:\n",
    "        # no numeric predictors -> fall back to global mode per column\n",
    "        X_num = None\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        miss_mask = out[col].isna() | (out[col].astype(str).str.strip().eq(\"\"))\n",
    "        if not miss_mask.any():\n",
    "            continue\n",
    "\n",
    "        if X_num is None:\n",
    "            # purely fallback: fill with mode\n",
    "            mode_val = out[col].mode(dropna=True).iloc[0] if out[col].notna().any() else \"MISSING\"\n",
    "            out.loc[miss_mask, col] = mode_val\n",
    "            continue\n",
    "\n",
    "        donors_mask = ~miss_mask & out[col].notna()\n",
    "        if donors_mask.sum() == 0:\n",
    "            out.loc[miss_mask, col] = \"MISSING\"\n",
    "            continue\n",
    "\n",
    "        # Fit NN on donors only\n",
    "        nn = NearestNeighbors(n_neighbors=min(k, donors_mask.sum()), metric=\"euclidean\")\n",
    "        nn.fit(X_num[donors_mask.values, :])\n",
    "\n",
    "        # Query neighbours for all missing rows at once\n",
    "        dist, idx = nn.kneighbors(X_num[miss_mask.values, :], return_distance=True)\n",
    "        donor_idx = np.where(donors_mask)[0]  # indices of donors in original df\n",
    "\n",
    "        # For each missing row, pick the most frequent category among neighbours\n",
    "        imputed = []\n",
    "        for row in range(idx.shape[0]):\n",
    "            neighbour_rows = donor_idx[idx[row]]\n",
    "            cats = out[col].iloc[neighbour_rows].astype(object).values\n",
    "            # mode; break ties by first occurrence\n",
    "            vals, counts = np.unique(cats, return_counts=True)\n",
    "            imputed.append(vals[np.argmax(counts)])\n",
    "        out.loc[miss_mask, col] = imputed\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------- PMM imputation for CONTINUOUS (single pass) ----------\n",
    "def pmm_impute_continuous(\n",
    "    df: pd.DataFrame,\n",
    "    cont_cols: List[str],\n",
    "    feature_matrix: pd.DataFrame,\n",
    "    k: int = 5,\n",
    "    alpha: float = 1.0,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predictive Mean Matching (single imputation) per continuous variable:\n",
    "      1) Fit Ridge on observed rows: y ~ features\n",
    "      2) Compute predictive means for observed (donor) and missing cases\n",
    "      3) For each missing case, find k donors with closest predicted means\n",
    "         and randomly draw a donor's ACTUAL observed y as the imputed value.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    out = df.copy()\n",
    "\n",
    "    # Prepare features: impute any missing with median and standardize\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    X_imp = imp.fit_transform(feature_matrix.values)\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X_imp)\n",
    "\n",
    "    X_std_df = pd.DataFrame(X_std, index=feature_matrix.index, columns=feature_matrix.columns)\n",
    "\n",
    "    for col in cont_cols:\n",
    "        miss_mask = out[col].isna()\n",
    "        if not miss_mask.any():\n",
    "            continue\n",
    "\n",
    "        obs_mask = ~miss_mask\n",
    "        y_obs = out.loc[obs_mask, col].astype(float).values\n",
    "        if len(y_obs) < max(10, k + 1):\n",
    "            # too few donors: fill with median\n",
    "            med = np.nanmedian(y_obs) if len(y_obs) else 0.0\n",
    "            out.loc[miss_mask, col] = med\n",
    "            continue\n",
    "\n",
    "        X_obs = X_std_df.loc[obs_mask, :]\n",
    "        X_mis = X_std_df.loc[miss_mask, :]\n",
    "\n",
    "        # Fit predictive model\n",
    "        model = Ridge(alpha=alpha, random_state=random_state)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            model.fit(X_obs, y_obs)\n",
    "\n",
    "        # Predicted means\n",
    "        mu_obs = model.predict(X_obs)\n",
    "        mu_mis = model.predict(X_mis)\n",
    "\n",
    "        # For each missing row, find k nearest donors in prediction space\n",
    "        imputed_vals = []\n",
    "        for mu in mu_mis:\n",
    "            # distances in predictive means\n",
    "            d = np.abs(mu_obs - mu)\n",
    "            nn_idx = np.argpartition(d, kth=min(k-1, len(d)-1))[:k]\n",
    "            pick = rng.choice(nn_idx)\n",
    "            imputed_vals.append(y_obs[pick])\n",
    "\n",
    "        out.loc[miss_mask, col] = imputed_vals\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------- K-fold Target Encoding (leakage-safe) ----------\n",
    "def kfold_target_encode(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    target_col: str,\n",
    "    n_splits: int = 5,\n",
    "    smoothing: float = 10.0,\n",
    "    random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    K-fold target mean with smoothing (per category):\n",
    "      TE = (cat_mean * count + global_mean * smoothing) / (count + smoothing)\n",
    "    Computed in out-of-fold fashion to avoid leakage.\n",
    "    \"\"\"\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"Target column '{target_col}' not in DataFrame.\")\n",
    "\n",
    "    y = df[target_col].astype(float).values\n",
    "    global_mean = float(np.nanmean(y))\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # For stratification, coerce y to 0/1 if it's continuous but bounded:\n",
    "    strat_y = pd.cut(y, bins=[-1e9, 0.5, 1e9], labels=[0,1]).astype(int) if len(np.unique(y)) > 2 else y\n",
    "\n",
    "    for col in cat_cols:\n",
    "        s = df[col].astype(str).fillna(\"MISSING\")\n",
    "        enc = np.zeros(len(df), dtype=float)\n",
    "\n",
    "        for tr_idx, va_idx in skf.split(np.zeros(len(df)), strat_y):\n",
    "            # Means on training fold\n",
    "            train_y = y[tr_idx]\n",
    "            train_s = s.iloc[tr_idx]\n",
    "            cnt = train_s.value_counts()\n",
    "            mean = train_s.groupby(train_s).apply(lambda idx: np.mean(train_y[train_s==idx.name]))\n",
    "            # smoothed means\n",
    "            smoothed = ((mean * cnt) + global_mean * smoothing) / (cnt + smoothing)\n",
    "\n",
    "            # map to validation\n",
    "            enc[va_idx] = s.iloc[va_idx].map(smoothed).fillna(global_mean).values\n",
    "\n",
    "        out[f\"{col}__te\"] = enc\n",
    "\n",
    "    return out\n",
    "\n",
    "# ======================= MAIN PIPELINE =======================\n",
    "if __name__ == \"__main__\":\n",
    "    # ---- load & merge ----\n",
    "    files = glob.glob(RAW_GLOB)\n",
    "    datasets = {}\n",
    "    for f in files:\n",
    "        name = Path(f).stem\n",
    "        df0 = pd.read_csv(f, low_memory=False)\n",
    "        df0.columns = [c.strip() for c in df0.columns]\n",
    "        datasets[name] = df0\n",
    "\n",
    "    merged = merge_on_keys(list(datasets.values()), ID_COLS) if datasets else pd.DataFrame(columns=ID_COLS)\n",
    "    df = merged.copy()\n",
    "\n",
    "    # ---- infer types ----\n",
    "    if not CATEGORICALS or not NUMERICALS:\n",
    "        cats, nums = infer_types(df, ID_COLS)\n",
    "        if not CATEGORICALS: CATEGORICALS = cats\n",
    "        if not NUMERICALS:   NUMERICALS   = nums\n",
    "\n",
    "    # ---- 1) KNN imputation for categorical features ----\n",
    "    # Work on a copy to avoid modifying the original\n",
    "    df_cats_imputed = knn_impute_categoricals(df, CATEGORICALS, NUMERICALS, k=KNN_K)\n",
    "\n",
    "    # ---- 2) PMM imputation for continuous features ----\n",
    "    # Build a feature matrix for PMM using: numeric (median-filled) + *temporary* OHE of imputed categoricals\n",
    "    #   (OHE here is only to supply predictive signals for PMM; the final OHE is built again later)\n",
    "    if CATEGORICALS:\n",
    "        ohe_tmp = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        X_cats_tmp = pd.DataFrame(\n",
    "            ohe_tmp.fit_transform(df_cats_imputed[CATEGORICALS]),\n",
    "            index=df_cats_imputed.index\n",
    "        )\n",
    "    else:\n",
    "        X_cats_tmp = pd.DataFrame(index=df_cats_imputed.index)\n",
    "\n",
    "    # numeric block (median for PMM predictors)\n",
    "    if NUMERICALS:\n",
    "        num_imp_for_feat = SimpleImputer(strategy=\"median\")\n",
    "        X_nums_tmp = pd.DataFrame(num_imp_for_feat.fit_transform(df_cats_imputed[NUMERICALS]),\n",
    "                                  columns=NUMERICALS, index=df_cats_imputed.index)\n",
    "    else:\n",
    "        X_nums_tmp = pd.DataFrame(index=df_cats_imputed.index)\n",
    "\n",
    "    feat_for_pmm = pd.concat([X_nums_tmp, X_cats_tmp], axis=1)\n",
    "\n",
    "    df_cont_imputed = pmm_impute_continuous(\n",
    "        df_cats_imputed,\n",
    "        cont_cols=NUMERICALS,\n",
    "        feature_matrix=feat_for_pmm,\n",
    "        k=PMM_K,\n",
    "        alpha=PMM_ALPHA,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # ---- 3) Final encodings: One-Hot + Target Encoding ----\n",
    "    # One-Hot (final, machine-readable; avoids ordinal distortion)\n",
    "    if CATEGORICALS:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        X_ohe = pd.DataFrame(\n",
    "            ohe.fit_transform(df_cont_imputed[CATEGORICALS]),\n",
    "            index=df_cont_imputed.index,\n",
    "            columns=ohe.get_feature_names_out(CATEGORICALS)\n",
    "        )\n",
    "    else:\n",
    "        X_ohe = pd.DataFrame(index=df_cont_imputed.index)\n",
    "\n",
    "    # Target encoding (K-fold, smoothed). Only if TARGET present:\n",
    "    if TARGET in df_cont_imputed.columns:\n",
    "        te_frame = kfold_target_encode(df_cont_imputed, CATEGORICALS, TARGET, n_splits=5, smoothing=10.0, random_state=RANDOM_SEED)\n",
    "    else:\n",
    "        te_frame = pd.DataFrame(index=df_cont_imputed.index)\n",
    "\n",
    "    # Assemble final table\n",
    "    keep_cols = ID_COLS + ([TARGET] if TARGET in df_cont_imputed.columns else [])\n",
    "    base = df_cont_imputed[keep_cols + NUMERICALS].reset_index(drop=True)\n",
    "    X_final = pd.concat([base, X_ohe.reset_index(drop=True), te_frame.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Persist\n",
    "    out_path = OUT_DIR / \"01_preprocessed_knn_pmm_ohe_te.csv\"\n",
    "    X_final.to_csv(out_path, index=False)\n",
    "    print(f\"Saved -> {out_path.resolve()}\")\n",
    "\n",
    "    # Minimal audit\n",
    "    n_before = len(df)\n",
    "    n_after  = len(X_final)\n",
    "    miss_report = X_final.isna().sum().sort_values(ascending=False).head(10)\n",
    "    print(f\"Rows: {n_before} -> {n_after}\")\n",
    "    print(\"Top remaining missing (should be 0):\\n\", miss_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
