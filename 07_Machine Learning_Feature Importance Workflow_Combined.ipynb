{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ml_plus_feature_importance.py\n",
    "# ------------------------------------------------------------------\n",
    "# One-pass pipeline:\n",
    "# 1) HPO with Optuna + 5-fold CV (AUC) for 9 models\n",
    "# 2) 70/30 hold-out evaluation, full metrics, ROC, DeLong vs LGBM\n",
    "# 3) Refit 3 diverse models with their BEST params:\n",
    "#       - HistGradientBoostingClassifier  -> \"Gradient Boosting\"\n",
    "#       - RandomForestClassifier          -> \"Random Forest\"\n",
    "#       - SVC (RBF, probability=True)     -> \"Support Vector Machine\"\n",
    "#    Compute permutation importances (ROCâ€“AUC), build cross-model table,\n",
    "#    plot TOP-50 scatter + PDPs (1D & 2D) on the hold-out set.\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "from __future__ import annotations\n",
    "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------ Dependencies ------------------------\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, average_precision_score,\n",
    "    f1_score, accuracy_score, balanced_accuracy_score,\n",
    "    confusion_matrix, cohen_kappa_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier,\n",
    "    BaggingClassifier, HistGradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "\n",
    "# ------------------------- BASIC CONFIG -------------------------\n",
    "CSV_PATH    = \"normalised_with_binary_cai.csv\"   \n",
    "TARGET_COL  = \"binary_cai\"                                  \n",
    "POS_LABEL   = 1\n",
    "RANDOM_SEED = 12345\n",
    "N_SPLITS    = 5\n",
    "N_TRIALS    = 60        \n",
    "N_JOBS      = -1\n",
    "\n",
    "OUT = Path(\"ml_one_by_one\"); OUT.mkdir(exist_ok=True)\n",
    "(OUT/\"reports\").mkdir(exist_ok=True); (OUT/\"figs\").mkdir(exist_ok=True); (OUT/\"preds\").mkdir(exist_ok=True)\n",
    "FI_OUT = Path(\"feature_importance_outputs\"); FI_OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# ========================= LOAD + SPLIT =========================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"{TARGET_COL} not found in data.\")\n",
    "y = df[TARGET_COL].astype(int).values\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=[np.number, \"bool\"]).columns.tolist()\n",
    "\n",
    "# Preprocessors\n",
    "prep_tree = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "prep_linear = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"scaler\", StandardScaler(with_mean=True))]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols)\n",
    "], remainder=\"drop\")\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# ============================ Utils =============================\n",
    "def fit_transform_fold(prep: ColumnTransformer, X_train, X_valid):\n",
    "    P = prep\n",
    "    Xt = P.fit_transform(X_train)\n",
    "    Xv = P.transform(X_valid)\n",
    "    return P, Xt, Xv\n",
    "\n",
    "def metrics_from_cm(cm: np.ndarray) -> Dict[str, float]:\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    eps = 1e-12\n",
    "    sens = tp / (tp + fn + eps)\n",
    "    spec = tn / (tn + fp + eps)\n",
    "    ppv  = tp / (tp + fp + eps)\n",
    "    npv  = tn / (tn + fn + eps)\n",
    "    prev = (tp + fn) / (tp + tn + fp + fn + eps)\n",
    "    det_rate = tp / (tp + tn + fp + fn + eps)\n",
    "    det_prev = (tp + fp) / (tp + tn + fp + fn + eps)\n",
    "    return {\"Sensitivity\": sens, \"Specificity\": spec,\n",
    "            \"PosPredValue\": ppv, \"NegPredValue\": npv,\n",
    "            \"Prevalence\": prev, \"DetectionRate\": det_rate,\n",
    "            \"DetectionPrevalence\": det_prev, \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp}\n",
    "\n",
    "# scores for holdout\n",
    "def get_proba_fitted(prep: ColumnTransformer, clf, X):\n",
    "    X_ = prep.transform(X)\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        return clf.predict_proba(X_)[:, 1]\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        s = clf.decision_function(X_)\n",
    "        return (s - s.min()) / (s.max() - s.min() + 1e-12)\n",
    "    raise RuntimeError(\"Classifier has neither predict_proba nor decision_function.\")\n",
    "\n",
    "# ---------- DeLong (paired AUC) ----------\n",
    "def _compute_midrank(x):\n",
    "    J = np.argsort(x); Z = x[J]; N = len(x); T = np.zeros(N); i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5 * (i + j - 1) + 1\n",
    "        i = j\n",
    "    T2 = np.empty(N); T2[J] = T; return T2\n",
    "\n",
    "def _fast_delong(pred_sorted_transposed: np.ndarray, label_1_count: int):\n",
    "    m = label_1_count; n = pred_sorted_transposed.shape[1] - m\n",
    "    pos_preds = pred_sorted_transposed[:, :m]; neg_preds = pred_sorted_transposed[:, m:]\n",
    "    tx = np.apply_along_axis(_compute_midrank, 1, pos_preds)\n",
    "    ty = np.apply_along_axis(_compute_midrank, 1, neg_preds)\n",
    "    tz = np.apply_along_axis(_compute_midrank, 1, pred_sorted_transposed)\n",
    "    aucs = (tz[:, :m].sum(axis=1) / (m * n)) - (m + 1) / (2 * n)\n",
    "    v01 = (tx / n) - (tz[:, :m] / n)\n",
    "    v10 = 1 - (ty / m) + (tz[:, m:] / m)\n",
    "    sx = np.cov(v01); sy = np.cov(v10); s = sx / m + sy / n\n",
    "    return aucs, s\n",
    "\n",
    "def _auc_cov(y_true: np.ndarray, probs: np.ndarray):\n",
    "    order = np.argsort(-probs[:,0]); y = y_true[order]; preds = probs[order].T\n",
    "    m = int(y.sum()); aucs, cov = _fast_delong(preds, m); return aucs, cov\n",
    "\n",
    "def delong_test(y_true: np.ndarray, score_ref: np.ndarray, score_cmp: np.ndarray):\n",
    "    aucs, cov = _auc_cov(y_true.astype(int), np.vstack([score_ref, score_cmp]).T)\n",
    "    diff = aucs[0] - aucs[1]; var = cov[0,0] + cov[1,1] - 2*cov[0,1]\n",
    "    z = diff / max(np.sqrt(var), 1e-12); p = 2 * stats.norm.sf(abs(z))\n",
    "    return aucs[0], aucs[1], z, p\n",
    "\n",
    "# ---------- CV objective ----------\n",
    "def cv_auc(prep_kind: str, make_clf, params: dict) -> float:\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "    aucs = []\n",
    "    for tr_idx, va_idx in skf.split(X_tr, y_tr):\n",
    "        Xtr, Xva = X_tr.iloc[tr_idx], X_tr.iloc[va_idx]\n",
    "        ytr, yva = y_tr[tr_idx], y_tr[va_idx]\n",
    "        prep = prep_tree if prep_kind == \"tree\" else prep_linear\n",
    "        P, Xtr_, Xva_ = fit_transform_fold(prep, Xtr, Xva)\n",
    "        clf = make_clf(params)\n",
    "        # Early stopping for boosters\n",
    "        if isinstance(clf, LGBMClassifier):\n",
    "            clf.set_params(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "            clf.fit(Xtr_, ytr, eval_set=[(Xva_, yva)], eval_metric=\"auc\", verbose=-1)\n",
    "        elif isinstance(clf, XGBClassifier):\n",
    "            clf.set_params(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "            clf.fit(Xtr_, ytr, eval_set=[(Xva_, yva)], eval_metric=\"auc\",\n",
    "                    early_stopping_rounds=100, verbose=False)\n",
    "        else:\n",
    "            clf.fit(Xtr_, ytr)\n",
    "        s = clf.predict_proba(Xva_)[:, 1] if hasattr(clf,\"predict_proba\") else \\\n",
    "            (lambda d: (d-d.min())/(d.max()-d.min()+1e-12))(clf.decision_function(Xva_))\n",
    "        aucs.append(roc_auc_score(yva, s))\n",
    "    return float(np.mean(aucs))\n",
    "\n",
    "# ===================== Search spaces (same as before) =====================\n",
    "def optimize_HGB():\n",
    "    def make(params):\n",
    "        return HistGradientBoostingClassifier(\n",
    "            learning_rate=params[\"lr\"], max_depth=params[\"max_depth\"],\n",
    "            max_leaf_nodes=params[\"max_leaf_nodes\"], min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "            l2_regularization=params[\"l2\"], max_bins=params[\"max_bins\"], random_state=RANDOM_SEED)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"lr\": trial.suggest_float(\"lr\", 0.01, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20),\n",
    "            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 16, 512, log=True),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 10, 200, log=True),\n",
    "            \"l2\": trial.suggest_float(\"l2\", 1e-8, 10.0, log=True),\n",
    "            \"max_bins\": trial.suggest_int(\"max_bins\", 64, 255)\n",
    "        }\n",
    "        return cv_auc(\"tree\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "def optimize_LGBM():\n",
    "    def make(params):\n",
    "        return LGBMClassifier(\n",
    "            objective=\"binary\", boosting_type=\"gbdt\",\n",
    "            n_estimators=params[\"n_estimators\"], learning_rate=params[\"lr\"],\n",
    "            num_leaves=params[\"num_leaves\"], max_depth=params[\"max_depth\"],\n",
    "            min_child_samples=params[\"min_child_samples\"],\n",
    "            subsample=params[\"subsample\"], colsample_bytree=params[\"colsample\"],\n",
    "            reg_lambda=params[\"l2\"], reg_alpha=params[\"l1\"],\n",
    "            random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 3000, log=True),\n",
    "            \"lr\": trial.suggest_float(\"lr\", 0.005, 0.3, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 512, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", -1, 32),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 200, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample\": trial.suggest_float(\"colsample\", 0.6, 1.0),\n",
    "            \"l2\": trial.suggest_float(\"l2\", 1e-8, 10.0, log=True),\n",
    "            \"l1\": trial.suggest_float(\"l1\", 1e-8, 10.0, log=True)\n",
    "        }\n",
    "        return cv_auc(\"tree\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "def optimize_ETC():\n",
    "    def make(params):\n",
    "        return ExtraTreesClassifier(\n",
    "            n_estimators=params[\"n_estimators\"], max_depth=params[\"max_depth\"],\n",
    "            max_features=params[\"max_features\"], min_samples_split=params[\"min_split\"],\n",
    "            min_samples_leaf=params[\"min_leaf\"], bootstrap=params[\"bootstrap\"],\n",
    "            random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 40),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.2, 1.0),\n",
    "            \"min_split\": trial.suggest_int(\"min_split\", 2, 50, log=True),\n",
    "            \"min_leaf\": trial.suggest_int(\"min_leaf\", 1, 50, log=True),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [False, True])\n",
    "        }\n",
    "        return cv_auc(\"tree\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "def optimize_RF():\n",
    "    def make(params):\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=params[\"n_estimators\"], max_depth=params[\"max_depth\"],\n",
    "            max_features=params[\"max_features\"], min_samples_split=params[\"min_split\"],\n",
    "            min_samples_leaf=params[\"min_leaf\"], bootstrap=params[\"bootstrap\"],\n",
    "            random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 40),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.2, 1.0),\n",
    "            \"min_split\": trial.suggest_int(\"min_split\", 2, 50, log=True),\n",
    "            \"min_leaf\": trial.suggest_int(\"min_leaf\", 1, 50, log=True),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "        }\n",
    "        return cv_auc(\"tree\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "def optimize_XGB():\n",
    "    def make(params):\n",
    "        return XGBClassifier(\n",
    "            objective=\"binary:logistic\", eval_metric=\"auc\",\n",
    "            n_estimators=params[\"n_estimators\"], learning_rate=params[\"lr\"],\n",
    "            max_depth=params[\"max_depth\"], subsample=params[\"subsample\"],\n",
    "            colsample_bytree=params[\"colsample_bytree\"], reg_lambda=params[\"l2\"],\n",
    "            reg_alpha=params[\"l1\"], min_child_weight=params[\"min_child_weight\"],\n",
    "            gamma=params[\"gamma\"], random_state=RANDOM_SEED, n_jobs=N_JOBS, use_label_encoder=False)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 3000, log=True),\n",
    "            \"lr\": trial.suggest_float(\"lr\", 0.005, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 16),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"l2\": trial.suggest_float(\"l2\", 1e-8, 10.0, log=True),\n",
    "            \"l1\": trial.suggest_float(\"l1\", 1e-8, 10.0, log=True),\n",
    "            \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0)\n",
    "        }\n",
    "        return cv_auc(\"tree\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "def optimize_LinearSVC():\n",
    "    def make(params):\n",
    "        base = LinearSVC(C=params[\"C\"], tol=params[\"tol\"], max_iter=20000)\n",
    "        return CalibratedClassifierCV(base, cv=3, method=\"sigmoid\")\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 1e-3, 100.0, log=True),\n",
    "            \"tol\": trial.suggest_float(\"tol\", 1e-5, 1e-2, log=True)\n",
    "        }\n",
    "        return cv_auc(\"linear\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "def optimize_Bagging():\n",
    "    def make(params):\n",
    "        return BaggingClassifier(\n",
    "            n_estimators=params[\"n_estimators\"], max_samples=params[\"max_samples\"],\n",
    "            max_features=params[\"max_features\"], bootstrap=params[\"bootstrap\"],\n",
    "            bootstrap_features=params[\"bootstrap_features\"], random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000, log=True),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\", [True, False])\n",
    "        }\n",
    "        return cv_auc(\"tree\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "def optimize_SVC():\n",
    "    def make(params):\n",
    "        return SVC(C=params[\"C\"], gamma=params[\"gamma\"], kernel=\"rbf\",\n",
    "                   probability=True, random_state=RANDOM_SEED)\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 1e-2, 1e3, log=True),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 1e-4, 1e1, log=True)\n",
    "        }\n",
    "        return cv_auc(\"linear\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "def optimize_LogReg():\n",
    "    def make(params):\n",
    "        return LogisticRegression(\n",
    "            C=params[\"C\"], penalty=params[\"penalty\"], solver=params[\"solver\"],\n",
    "            max_iter=20000, n_jobs=N_JOBS)\n",
    "    def objective(trial):\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\", \"l1\"])\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"lbfgs\", \"liblinear\", \"saga\"])\n",
    "        if penalty == \"l1\" and solver not in [\"liblinear\", \"saga\"]:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 1e-3, 100.0, log=True),\n",
    "            \"penalty\": penalty,\n",
    "            \"solver\": solver\n",
    "        }\n",
    "        return cv_auc(\"linear\", make, params)\n",
    "    st = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    st.optimize(objective, n_trials=N_TRIALS)\n",
    "    return st, make\n",
    "\n",
    "# ======================= RUN EACH MODEL =======================\n",
    "optimizers = [\n",
    "    (\"HistGradientBoostingClassifier\", optimize_HGB, \"tree\"),\n",
    "    (\"LGBMClassifier\",                  optimize_LGBM, \"tree\"),\n",
    "    (\"ExtraTreesClassifier\",            optimize_ETC, \"tree\"),\n",
    "    (\"RandomForestClassifier\",          optimize_RF,  \"tree\"),\n",
    "    (\"XGBClassifier\",                   optimize_XGB, \"tree\"),\n",
    "    (\"LinearSVC\",                       optimize_LinearSVC, \"linear\"),\n",
    "    (\"BaggingClassifier\",               optimize_Bagging, \"tree\"),\n",
    "    (\"SVC\",                             optimize_SVC, \"linear\"),\n",
    "    (\"LogisticRegression\",              optimize_LogReg, \"linear\"),\n",
    "]\n",
    "\n",
    "all_metrics, roc_curves, proba_cache = [], {}, {}\n",
    "best_params_by_model: Dict[str, dict] = {}\n",
    "prep_kind_by_model: Dict[str, str] = {}\n",
    "\n",
    "for model_name, opt_fun, prep_kind in optimizers:\n",
    "    print(f\"\\n=== {model_name}: HPO with 5-fold CV ===\")\n",
    "    study, maker = opt_fun()\n",
    "    best_params = study.best_trial.params\n",
    "    best_params_by_model[model_name] = best_params\n",
    "    prep_kind_by_model[model_name] = prep_kind\n",
    "    pd.DataFrame([best_params]).to_csv(OUT/\"reports\"/f\"{model_name}_best_params.csv\", index=False)\n",
    "\n",
    "    # Fit on training set (with small internal val split for boosters)\n",
    "    P = prep_tree if prep_kind == \"tree\" else prep_linear\n",
    "    P.fit(X_tr); Xtr_ = P.transform(X_tr); Xte_ = P.transform(X_te)\n",
    "    clf = maker(best_params)\n",
    "\n",
    "    if isinstance(clf, LGBMClassifier):\n",
    "        Xtr_i, Xva_i, ytr_i, yva_i = train_test_split(Xtr_, y_tr, test_size=0.1,\n",
    "                                                      stratify=y_tr, random_state=RANDOM_SEED)\n",
    "        clf.set_params(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "        clf.fit(Xtr_i, ytr_i, eval_set=[(Xva_i, yva_i)], eval_metric=\"auc\", verbose=-1)\n",
    "    elif isinstance(clf, XGBClassifier):\n",
    "        Xtr_i, Xva_i, ytr_i, yva_i = train_test_split(Xtr_, y_tr, test_size=0.1,\n",
    "                                                      stratify=y_tr, random_state=RANDOM_SEED)\n",
    "        clf.set_params(random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "        clf.fit(Xtr_i, ytr_i, eval_set=[(Xva_i, yva_i)], eval_metric=\"auc\",\n",
    "                early_stopping_rounds=100, verbose=False)\n",
    "    else:\n",
    "        clf.fit(Xtr_, y_tr)\n",
    "\n",
    "    s_test = clf.predict_proba(Xte_)[:, 1] if hasattr(clf,\"predict_proba\") else \\\n",
    "             (lambda d: (d-d.min())/(d.max()-d.min()+1e-12))(clf.decision_function(Xte_))\n",
    "    y_pred = (s_test >= 0.5).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(y_te, s_test)\n",
    "    ap  = average_precision_score(y_te, s_test)\n",
    "    f1  = f1_score(y_te, y_pred)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_te, y_pred)\n",
    "    kap = cohen_kappa_score(y_te, y_pred)\n",
    "    cm  = confusion_matrix(y_te, y_pred, labels=[0,1])\n",
    "    more = metrics_from_cm(cm)\n",
    "    row = {\"Model\": model_name, \"AUC\": auc, \"AP\": ap, \"F1\": f1,\n",
    "           \"BalancedAcc\": bacc, \"Kappa\": kap, \"Accuracy\": acc, **more}\n",
    "    all_metrics.append(row)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_te, s_test)\n",
    "    roc_curves[model_name] = (fpr, tpr, auc)\n",
    "\n",
    "    proba_cache[model_name] = s_test\n",
    "    np.save(OUT/\"preds\"/f\"{model_name}_test_scores.npy\", s_test)\n",
    "    print(f\"{model_name} | AUC {auc:.4f}  AP {ap:.4f}\")\n",
    "\n",
    "# Save metrics + ROC\n",
    "metrics_df = pd.DataFrame(all_metrics).sort_values(\"AUC\", ascending=False)\n",
    "metrics_df.to_csv(OUT/\"reports\"/\"metrics_table.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for name,(fpr,tpr,auc) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{name} [AUC {auc:.3f}]\")\n",
    "plt.plot([0,1],[0,1],\"k--\",lw=1)\n",
    "plt.xlabel(\"False Positive Rate [1 - Specificity]\"); plt.ylabel(\"True Positive Rate [Sensitivity]\")\n",
    "plt.title(\"ROC-AUC Curve (70/30 hold-out)\")\n",
    "plt.legend(loc=\"lower right\", fontsize=8); plt.tight_layout()\n",
    "plt.savefig(OUT/\"figs\"/\"roc_auc_holdout.png\", dpi=220); plt.close()\n",
    "\n",
    "# DeLong vs LGBM\n",
    "if \"LGBMClassifier\" in proba_cache:\n",
    "    y_true = y_te.astype(int)\n",
    "    ref = proba_cache[\"LGBMClassifier\"]\n",
    "    rows = []\n",
    "    for name, s in proba_cache.items():\n",
    "        if name == \"LGBMClassifier\": continue\n",
    "        auc_ref, auc_cmp, z, p = delong_test(y_true, ref, s)\n",
    "        rows.append({\"Model\": name, \"AUC_ref(LGBM)\": auc_ref, \"AUC_model\": auc_cmp,\n",
    "                     \"stat z\": z, \"p-value\": p})\n",
    "    delong_df = pd.DataFrame(rows).sort_values(\"p-value\")\n",
    "    delong_df.to_csv(OUT/\"reports\"/\"delong_vs_LGBM.csv\", index=False)\n",
    "    print(\"\\nDeLong vs LGBM:\\n\", delong_df)\n",
    "else:\n",
    "    print(\"Skipped DeLong: LGBMClassifier not available.\")\n",
    "\n",
    "print(f\"\\nAll ML artifacts saved in: {OUT.resolve()}\")\n",
    "\n",
    "# ================== FEATURE IMPORTANCE (driven by HPO outputs) ==================\n",
    "# refitting the 3 diverse models using their BEST params found above,\n",
    "# build Pipelines (preprocessor + estimator), then compute permutation importances.\n",
    "\n",
    "# Friendly labels mapping\n",
    "friendly = {\n",
    "    \"HistGradientBoostingClassifier\": \"Gradient Boosting\",\n",
    "    \"RandomForestClassifier\": \"Random Forest\",\n",
    "    \"SVC\": \"Support Vector Machine\"\n",
    "}\n",
    "\n",
    "# Pick models that exist in best_params_by_model\n",
    "fi_models = []\n",
    "for key in [\"HistGradientBoostingClassifier\", \"RandomForestClassifier\", \"SVC\"]:\n",
    "    if key in best_params_by_model:\n",
    "        params = best_params_by_model[key]\n",
    "        kind   = prep_kind_by_model[key]\n",
    "        prep   = prep_tree if kind == \"tree\" else prep_linear\n",
    "        if key == \"HistGradientBoostingClassifier\":\n",
    "            est = HistGradientBoostingClassifier(**params, random_state=RANDOM_SEED)\n",
    "        elif key == \"RandomForestClassifier\":\n",
    "            est = RandomForestClassifier(**params, random_state=RANDOM_SEED, n_jobs=N_JOBS)\n",
    "        elif key == \"SVC\":\n",
    "            est = SVC(**params, probability=True, random_state=RANDOM_SEED)\n",
    "        pipe = Pipeline([(\"prep\", prep), (\"clf\", est)])\n",
    "        fi_models.append((friendly[key], pipe))\n",
    "\n",
    "# Fit on train, evaluate AUC on holdout (used as weights)\n",
    "aucs_fi: Dict[str, float] = {}\n",
    "fitted_pipes: Dict[str, Pipeline] = {}\n",
    "for name, pipe in fi_models:\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    s = pipe.predict_proba(X_te)[:, 1] if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\") else \\\n",
    "        (lambda d:(d-d.min())/(d.max()-d.min()+1e-12))(pipe.decision_function(X_te))\n",
    "    aucs_fi[name] = roc_auc_score(y_te, s)\n",
    "    fitted_pipes[name] = pipe\n",
    "\n",
    "# Permutation importances (on original columns via Pipeline)\n",
    "imps: Dict[str, np.ndarray] = {}\n",
    "feat_names = np.array(X.columns)\n",
    "for name, pipe in fitted_pipes.items():\n",
    "    r = permutation_importance(pipe, X_te, y_te, scoring=\"roc_auc\",\n",
    "                               n_repeats=10, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    imp = r.importances_mean.clip(min=0)\n",
    "    if imp.max() > 0: imp = imp / imp.max()\n",
    "    imps[name] = imp\n",
    "\n",
    "# Build importance table\n",
    "imp_df_list = [pd.DataFrame({\"feature\": feat_names, \"model\": m, \"importance\": arr})\n",
    "               for m, arr in imps.items()]\n",
    "imp_long = pd.concat(imp_df_list, ignore_index=True)\n",
    "\n",
    "avg_imp = (imp_long.groupby(\"feature\")[\"importance\"].mean()\n",
    "           .rename(\"avg_importance\").reset_index())\n",
    "\n",
    "w_series = pd.Series(aucs_fi); w_series = w_series / w_series.sum()\n",
    "w = imp_long.merge(w_series.rename(\"w\").rename_axis(\"model\").reset_index(), on=\"model\", how=\"left\")\n",
    "wavg_imp = (w.assign(w_imp=lambda d: d[\"importance\"] * d[\"w\"])\n",
    "            .groupby(\"feature\")[\"w_imp\"].sum()\n",
    "            .rename(\"wavg_importance\").reset_index())\n",
    "\n",
    "imp_summary = (avg_imp.merge(wavg_imp, on=\"feature\")\n",
    "               .merge(imp_long.pivot(index=\"feature\", columns=\"model\", values=\"importance\").reset_index(),\n",
    "                      on=\"feature\", how=\"left\"))\\\n",
    "               .sort_values(\"avg_importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save & plot TOP-50\n",
    "TOP_K_FEATURES = 50\n",
    "imp_summary.to_csv(FI_OUT/\"feature_importances_all_models.csv\", index=False)\n",
    "top50 = imp_summary.head(TOP_K_FEATURES).copy()\n",
    "top50.to_csv(FI_OUT/f\"feature_importances_top{TOP_K_FEATURES}.csv\", index=False)\n",
    "\n",
    "# Scatter plot\n",
    "top_long = imp_long[imp_long[\"feature\"].isin(top50[\"feature\"])]\n",
    "avg_for_plot = top50[[\"feature\",\"avg_importance\"]].assign(model=\"Average\")\\\n",
    "                 .rename(columns={\"avg_importance\":\"importance\"})\n",
    "wavg_for_plot = top50[[\"feature\",\"wavg_importance\"]].assign(model=\"Average (weighted)\")\\\n",
    "                  .rename(columns={\"wavg_importance\":\"importance\"})\n",
    "top_long_plot = pd.concat([top_long, avg_for_plot, wavg_for_plot], ignore_index=True)\n",
    "cat_order = top50[\"feature\"].tolist()[::-1]\n",
    "\n",
    "palette = {\n",
    "    \"Average\": \"black\",\n",
    "    \"Average (weighted)\": \"dimgray\",\n",
    "    \"Gradient Boosting\": \"#ff7f0e\",\n",
    "    \"Random Forest\": \"#1f77b4\",\n",
    "    \"Support Vector Machine\": \"#2ca02c\",\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(9, 11))\n",
    "for model_name, sub in top_long_plot.groupby(\"model\"):\n",
    "    y_pos = sub[\"feature\"].apply(lambda f: cat_order.index(f))\n",
    "    plt.scatter(sub[\"importance\"], y_pos, s=22, label=model_name,\n",
    "                color=palette.get(model_name, None), alpha=0.95, edgecolor=\"none\")\n",
    "plt.yticks(ticks=range(len(cat_order)), labels=cat_order, fontsize=8)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importance\"); plt.ylabel(\"Features\")\n",
    "plt.title(f\"Feature importances across Models â€” Top {TOP_K_FEATURES}\")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "order = [\"Average\",\"Average (weighted)\",\"Gradient Boosting\",\"Random Forest\",\"Support Vector Machine\"]\n",
    "ordered = [handles[labels.index(l)] for l in order if l in labels]\n",
    "plt.legend(ordered, order, loc=\"center left\", bbox_to_anchor=(0.72, 0.5), title=\"Model\", frameon=False)\n",
    "plt.tight_layout(); plt.savefig(FI_OUT/f\"feature_importances_top{TOP_K_FEATURES}.png\", dpi=220); plt.close()\n",
    "\n",
    "print(f\"[OK] Saved feature-importance plots/tables -> {FI_OUT.resolve()}\")\n",
    "\n",
    "# ===================== PDPs (use best AUC among the three) =====================\n",
    "best_model_name = max(aucs_fi, key=aucs_fi.get)\n",
    "best_pipe = fitted_pipes[best_model_name]\n",
    "print(f\"PDPs using: {best_model_name}  AUC={aucs_fi[best_model_name]:.3f}\")\n",
    "\n",
    "# Configure PDP feature pairs (fallback to top-ranked)\n",
    "PDP_GROUPS: List[List[str]] = [\n",
    "    [\"gg_medie_irrig\", \"plv_colture\"],\n",
    "    [\"sau_irrigata\", \"volume_acqua_ha\"],\n",
    "    [\"share_irrigated\", \"irr_pioggia\"]\n",
    "]\n",
    "fallback_feats = top50[\"feature\"].tolist()\n",
    "all_feats = set(X.columns.tolist())\n",
    "\n",
    "def choose_pair(pair: List[str]) -> List[str]:\n",
    "    a, b = pair\n",
    "    if a in all_feats and b in all_feats: return pair\n",
    "    # fallback to first two distinct features from top50\n",
    "    out = []\n",
    "    for f in fallback_feats:\n",
    "        if f in all_feats and f not in out:\n",
    "            out.append(f)\n",
    "        if len(out) == 2: break\n",
    "    return out\n",
    "\n",
    "for grp in PDP_GROUPS:\n",
    "    f1, f2 = choose_pair(grp)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 3.7))\n",
    "    PartialDependenceDisplay.from_estimator(best_pipe, X, [f1], ax=axes[0], kind=\"average\", grid_resolution=30)\n",
    "    axes[0].set_ylabel(\"Target variable\")\n",
    "    PartialDependenceDisplay.from_estimator(best_pipe, X, [f2], ax=axes[1], kind=\"average\", grid_resolution=30)\n",
    "    axes[1].set_ylabel(\"Target variable\")\n",
    "    PartialDependenceDisplay.from_estimator(best_pipe, X, [(f1, f2)], ax=axes[2], kind=\"average\", grid_resolution=25)\n",
    "    fig.suptitle(f\"Features: {f1} and {f2}\", y=1.03, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    out_png = FI_OUT/f\"pdp_{f1}__{f2}.png\"\n",
    "    plt.savefig(out_png, dpi=220, bbox_inches=\"tight\"); plt.close()\n",
    "    print(f\"[OK] Saved PDP: {out_png.name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
